<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2017%2F03%2F22%2Ftest%2F</url>
    <content type="text"><![CDATA[123456st=&gt;start: Starte=&gt;end: Endop1=&gt;operation: My Operationsub1=&gt;subroutine: My Subroutinecond=&gt;condition: Yes or No?io=&gt;inputoutput: catch something...]]></content>
  </entry>
  <entry>
    <title><![CDATA[java kafka-client应用与http实现]]></title>
    <url>%2F2017%2F03%2F10%2Fkafka-java-client%2F</url>
    <content type="text"><![CDATA[使用kafka-client实现producer api与consumer api,并且提供简单的http接口 大多数情况下,我们使用kafka-connect来做kafka集群数据的收集与导出,使用kafka-steams api来做流式处理。在这之前我们先来了解一下java kafka client。kafka不是一个有序的消息队列,kafka不是一个有序的消息队列,kafka不是一个有序的消息队列,重要的事情说三遍。(￣▽￣;) producerkafka producer主要负责负载均衡和异步发送,大致的过程如下: producer连接至kafka之后,会获取kafka集群中的partitions信息保存在KafkaProducer的MetaData中并定时刷新,如下表: topic partitions broker type foo 0 1001 leader foo 1 1002 replication bar 49 1003 leader bar 50 1002 repilcation ProducerRecorder代表每条消息,包含四部分信息(key,value,topic,partitions,timestamp) 根据ProducerRecorder提供的topic,partitions信息,KafkaProducer可以定位到一个leader broker,KafkaProducer与leader broker建立连接,通过Future异步发送ProducerRecorder至leader broker,保存key,value信息到指定的partitions。 producer的负载均衡策略都是在客户端实现,ProducerRecorder类4个初始化参数非常重要,特别是partitions,该参数决定了负载如何分布。为了解决这个问题kafka-client提供了两种默认的partitions策略 如果key为null,则轮询partitions,保证负载尽量均匀,但是ProducerRecorder仅在单个partitions中有序 (￣▽￣;). 如果key存在,则将key通过hash算法确定一个partitions(此部分可自定义,实现Partitioner接口即可),此种方式负载并不均衡,但是因为相同的key会存在同一个partitions中,所以相同的key是有序的。 推荐第二种方式,实际项目中kafka大多存储一些数据信息,比如用户行为,网站日志,监控信息,这些都是可以定义key的,可以将user_id,session ID,appID, IP,作为key.当我们需要一个有序队列提供给consumer时,可以通过stream api将按key过滤,流化到另外一个只有一个partitions的topic中。 consumerkafka consumer 需要注意的几个地方 consumer从topic pull数据,注意!! 是拉数据,并不是服务端push过来的. consumer pull数据的时候,需要指定partitions,如果不指定partitions,并且topic有多个partitions,那么拉取数据所选择的partitions是随机的,随机的,随机的(￣▽￣;). 如果多个consumer指定同一个group.id,那么pull数据的时候,单个consumer与partitions绑定,也就是说如果同一个group.id的consumer数量大于partitions数量是没有意义的,超出partitions数量的consumer线程得不到任何数据. 同一个group.id共享partitions的offset信息,多线程情况下请手动更新 (￣▽￣;); consumer详细的过程如下: consumer连接至kafka之后,会获取kafka集群中的partitions信息保存在KafkaConsumer的MetaData中并定时刷新.此部分与producer相同. KafkaConsumer在subscribe会为每个topic维护类似下表的信息,logSize为partitions大小,offset为该group.id的目前的位置,lag为还剩下多少条数据。 Partition LogSize Consumer Offset lag 0 100 1 99 1 100 99 1 2 100 23 77 3 100 7 93 KafkaConsumer在pull时会根据subscribe信息定位到一个leader broker并建立连接,然后pull数据返回ConsumerRecord,pull的开始位置由上表的offset决定。pull完成之后,更新上表中的OffSet与lag信息(多线程情况下请手动commit)。 ConsumerRecord包含topic,key,value,timestamp,offset,partitions等信息。 consumer在pull多个partitions的究竟是先从哪个partitions pull数据?,这个问题kafka-client提供两种策略: 动态指定: assign a fair share of the partitions for those topics based on the active consumers in the group这个官方的说法,公平的选择partions,但是我没看到源码(￣▽￣;),测试来看貌似随机的. 手动指定:此种方式通过org.apache.kafka.clients.consumer.KafkaConsumer#assign方法实现,可以手动指定Paritions 推荐选择第一种方式,至于消息顺序问题,可以使用kafka stream api 解决。 为kafka集群搭建http接口kafka集群只有官方提供的java版本功能较为全面,其它语言有些是社区版本功能不完善,还有一些语言根本就没有client实现,比如php.此时对非java语言提供http接口访问kafka是最方便的,下面我们使用spring-boot来实现producer和consumer的http接口 原料 Docker version 1.13.0 docker-compose version 1.10.0 zookeeper 集群 搭建方式见&lt;使用docker创建zookeeper集群&gt; kafka 集群 搭建方式见&lt;使用docker创建kafka集群&gt; spring-boot 使用方式见&lt;spring-boot微框架创建java docker应用&gt; kafka-clients 0.10.1.1 接口设计设计接口的时候我们屏蔽了一些客户端不需要特别关注的细节,把这些细节处理放在http接口里面处理。 producer接口 GET /producer/{topics}/{key}/{value} RETURN {“topics”:topicList,”key”:key,”value”:value} 说明: producer客户端只需要关心{topics}{key}{value}即可,{topics}可以为多个topic,,分割。 consumer接口 GET /consumer/{topics}/{limit} RETURN {“result”,ConsumerRecordList,”topics”:topicList} 说明: consumer只需要提供{topics}{limit}即可,表明从那个topic取多少条数据,{topics}可以为多个topic,,分割。auto.offset.reset我们默认使用earliest取最早的数据,每次请求单独创建一个group.id(UUID) 创建spring boot项目1spring init 修改pom.xml添加依赖12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;1.1.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 说明：spring-kafka包含kafka-client 修改application.properties1234567891011#serverserver.port=9527#kafkakafka.global.bootstrap.servers=kafka1:9092,kafka2:9092,kafka3:9092kafka.producer.key.serializer=org.apache.kafka.common.serialization.StringSerializerkafka.producer.value.serializer=org.apache.kafka.common.serialization.StringSerializerkafka.producer.kakfa.global.compression.type=org.apache.kafka.common.serialization.StringSerializerkafka.consumer.key.deserializer=org.apache.kafka.common.serialization.StringDeserializerkafka.consumer.value.deserializer=org.apache.kafka.common.serialization.StringDeserializerkafka.consumer.auto.offset.reset=earliest 编写DemoApplication.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package com.example;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.CommandLineRunner;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.kafka.annotation.EnableKafka;import org.springframework.web.bind.annotation.*;import java.util.*;@SuppressWarnings("ALL")@SpringBootApplication@EnableAutoConfiguration@RestController@EnableKafkapublic class DemoApplication implements CommandLineRunner &#123; @Value("$&#123;kafka.global.bootstrap.servers&#125;") private String bootStrapServers; @Value("$&#123;kafka.producer.key.serializer&#125;") private String keySerializer; @Value("$&#123;kafka.producer.value.serializer&#125;") private String valueSerializer; @Value("$&#123;kafka.consumer.key.deserializer&#125;") private String keyDeserializer; @Value("$&#123;kafka.consumer.value.deserializer&#125;") private String valueDeserializer; @Value("$&#123;kafka.consumer.auto.offset.reset&#125;") private String autoOffsetReset; @RequestMapping(value = "/producer/&#123;topics&#125;/&#123;key&#125;/&#123;value&#125;") @ResponseBody public HashMap producer(@PathVariable(value = "topics") String topics,@PathVariable(value = "value") String value,@PathVariable(value = "key") String key) &#123; List&lt;String&gt; topicList = Arrays.asList(topics.split(",")); HashMap resultMap = new HashMap&lt;String,Object&gt;(); Properties producerProperties = getProducerProperties(); KafkaProducer&lt;String,String&gt; kafkaProducer = new KafkaProducer&lt;String,String&gt;(producerProperties); for (String topic : topicList) &#123; ProducerRecord&lt;String,String&gt; producerRecord = new ProducerRecord&lt;String,String&gt;(topic,key,value); kafkaProducer.send(producerRecord); &#125; kafkaProducer.close(); resultMap.put("topics",topicList); resultMap.put("value",value); resultMap.put("key",key); return resultMap; &#125; @RequestMapping(value = "/consumer/&#123;topics&#125;/&#123;limit&#125;") @ResponseBody public HashMap consumer(@PathVariable(value = "topics") String topics,@PathVariable("limit") String limit,@RequestParam(value = "group.id",defaultValue = "",required = false) String groupId) &#123; List&lt;String&gt; topicList = Arrays.asList(topics.split(",")); HashMap resultMap = new HashMap&lt;String,Object&gt;(); Properties consumerProperties = getConsumerProperties(); if (groupId.length() == 0) &#123; consumerProperties.put("group.id",UUID.randomUUID().toString()); &#125; else &#123; consumerProperties.put("group.id",groupId); &#125; consumerProperties.put("max.poll.records",limit); KafkaConsumer&lt;String,String&gt; kafkaConsumer = new KafkaConsumer&lt;String,String&gt;(consumerProperties); kafkaConsumer.subscribe(topicList); ConsumerRecords&lt;String,String&gt; consumerRecords = kafkaConsumer.poll(1000); resultMap.put("topics",topicList); List&lt;Map&gt; recordList = new ArrayList&lt;Map&gt;(); for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123; HashMap&lt;String,String&gt; recordMap = new HashMap&lt;&gt;(); recordMap.put("key",record.key()); recordMap.put("value",record.value()); recordMap.put("offset", String.valueOf(record.offset())); recordMap.put("partition", String.valueOf(record.partition())); recordMap.put("topic",record.topic()); recordMap.put("timestamp",String.valueOf(record.timestamp())); recordList.add(recordMap); &#125; resultMap.put("result",recordList); kafkaConsumer.close(); return resultMap; &#125; /** * 返回Producer配置 * @return */ private Properties getProducerProperties()&#123; Properties props = new Properties(); props.put("bootstrap.servers",this.bootStrapServers); props.put("key.serializer",this.keySerializer); props.put("value.serializer",this.valueSerializer); return props; &#125;; /** * 返回consumer配置 * @return */ private Properties getConsumerProperties()&#123; Properties props = new Properties(); props.put("bootstrap.servers",this.bootStrapServers); props.put("key.deserializer",this.keyDeserializer); props.put("value.deserializer",this.valueDeserializer); props.put("auto.offset.reset",this.autoOffsetReset); return props; &#125;; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125; @Override public void run(String... args) throws Exception &#123; System.out.println("服务启动"); &#125;&#125; ###clean &amp;&amp; package &amp;&amp; run1mvn clean package &amp;&amp; docker run --name="spring_boot" --rm --net=docker_default -v "$PWD/target/http-demo-0.0.1-SNAPSHOT.jar:/app/app.jar" -p '10086:9527' bankmonitor/spring-boot 启动之后我们的接口监听在10086端口 调用producer接口1repeat 1000 echo "curl http://127.0.0.1:10086/producer/foo,bar/"`random protocol`"/"`random ip` | bash 此命令会生成1000条数据,key为随机协议名,value为随机ip,并且把数据发送到foo,bar两个topic(请现在kafka中准备好这两个主题)。 调用consumer接口12➜ blog curl http://127.0.0.1:10086/consumer/foo,bar/10&#123;"result":[&#123;"partition":"6","offset":"0","topic":"foo","value":"27.123.34","key":"mid","timestamp":"1489545404505"&#125;,&#123;"partition":"6","offset":"1","topic":"foo","value":"226.104.141","key":"mid","timestamp":"1489545405196"&#125;,&#123;"partition":"6","offset":"2","topic":"foo","value":"169.71.224","key":"mid","timestamp":"1489545405660"&#125;,&#123;"partition":"6","offset":"3","topic":"foo","value":"39.25.19","key":"mid","timestamp":"1489545408197"&#125;,&#123;"partition":"6","offset":"4","topic":"foo","value":"48.106.188","key":"mid","timestamp":"1489545411873"&#125;,&#123;"partition":"6","offset":"5","topic":"foo","value":"1.234.228","key":"mid","timestamp":"1489545416681"&#125;,&#123;"partition":"6","offset":"6","topic":"foo","value":"60.231.223","key":"mid","timestamp":"1489545417588"&#125;,&#123;"partition":"6","offset":"7","topic":"foo","value":"112.170.18","key":"mid","timestamp":"1489545418957"&#125;,&#123;"partition":"6","offset":"8","topic":"foo","value":"132.159.29","key":"mid","timestamp":"1489545421914"&#125;,&#123;"partition":"6","offset":"9","topic":"foo","value":"201.12.222","key":"mid","timestamp":"1489545427370"&#125;],"topics":["foo","bar"],"total_count":10&#125;%]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>spring-boot</tag>
        <tag>java</tag>
        <tag>kafka-client</tag>
        <tag>queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot微框架创建java docker应用]]></title>
    <url>%2F2017%2F03%2F10%2Fdocker-spring-boot%2F</url>
    <content type="text"><![CDATA[使用sping-boot用最简单的方式发布一个包含http接口的docker服务。 目前项目上本地和测试环境中全部的中间件与server都是部署在docker环境,调试过程中需要将client端代码发布到docker container中运行,spring依赖注入的特性可以简化开发,但是xml配置非常繁琐,spring boot很好的解决了这个问题,下面我们用spring boot做一个简单的docker应用。 原料 Docker version 1.13.0 bankmonitor/spring-boot 镜像 spring boot v1.5.2.RELEASE使用spring boot微框架初始化项目初始化项目:123spring init http-demoUsing service at https://start.spring.ioProject extracted to '/Users/zhaoliang/project/http-demo' 该命令生成了一个包含spring基本组件的项目,目录结构不写了,标准的java项目,只说一下application.properties. 这个文件是spring boot的应用配置文件(说明见spring-boot-doc),我们添加一个server.port=9527,这个配置会修改嵌入tomcat的监听端口,application.properties中的配置内容可以通过@Value(&quot;{key}&quot;)在spring boot中引用,下面的例子会讲。 coding进入项目目录添加新类DemoApplication.java内容如下：12345678910111213141516171819202122232425262728package com.example;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@SpringBootApplication@EnableAutoConfiguration@RestControllerpublic class DemoApplication implements CommandLineRunner &#123; @Value("$&#123;server.port&#125;") public Integer port; @RequestMapping(value = "/hi") public String hi() &#123; return "helloWorld:" + this.port; &#125; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125; @Override public void run(String... args) throws Exception &#123; System.out.println("服务启动"); &#125;&#125; 说明:访问/hi服务端会返回”helloworld” 和我们在application.properties配置的端口信息.实现CommandLineRunner 在服务启动时候执行加载一些数据。 package &amp;&amp; run运行命令:12345678910111213mvn clean package[INFO][INFO] --- maven-jar-plugin:2.6:jar (default-jar) @ http-demo ---[INFO] Building jar: /Users/zhaoliang/project/http-demo/target/http-demo-0.0.1-SNAPSHOT.jar[INFO][INFO] --- spring-boot-maven-plugin:1.5.2.RELEASE:repackage (default) @ http-demo ---[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 4.690 s[INFO] Finished at: 2017-03-10T14:26:57+08:00[INFO] Final Memory: 28M/312M[INFO] ------------------------------------------------------------------------ ./target/http-demo-0.0.1-SNAPSHOT.jar为打包结果。 本地运行:1java -jar target/http-demo-0.0.1-SNAPSHOT.jar 查看结果:12curl http://127.0.0.1:9527/hihelloWorld:9527% 使用容器运行我们使用bankmonitor/spring-boot镜像来运行spring boot程序,镜像启动时候会执行如下cmd:1CMD [\"/bin/sh\" \"-c\" \"java $JAVA_OPTS -jar /app/app.jar --spring.profiles.active=$SPRING_PROFILES_ACTIVE\"] 这就简单了,我们只需要在容器启动时将./target/http-demo-0.0.1-SNAPSHOT.jar挂载到容器文件/app/app.jar即可运行命令:1docker run --rm -v "$PWD/target/http-demo-0.0.1-SNAPSHOT.jar:/app/app.jar" -p "10086:9527" bankmonitor/spring-boot 我们把容器9527端口映射到本地10086端口（´➰｀) 查看结果:12curl http://127.0.0.1:10086/hihelloWorld:10086% 结语spring-boot非常适合作为微框架,并且非常简单,简化了大分布的配置操作,再今后如有使用到新的特性与依赖,会再做分享。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>spring-boot</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用docker创建kafka集群]]></title>
    <url>%2F2017%2F03%2F02%2Fdocker-kafka-cluster%2F</url>
    <content type="text"><![CDATA[如何使用docker创建一个包含三个节点的kafka集群(非伪分布式),并提供web管理界面。 近期在项目上使用阿里datahub做存储用户行为并且做流式计算,kafka也是大数据解决方案中的一部分,接下来准备从kafka的环境搭建开始,分享一些相关内容。 原料 Docker version 1.13.0 docker-compose version 1.10.0 wurstmeister/kafka 镜像 服务端 ryane/kafkacat 镜像 客户端 sheepkiller/kafka-manager 镜像 管理工具kafka-manager是雅虎推出的kafka管理器偏重于集群与topic管理 zookeeper 集群 搭建方式见&lt;使用docker创建zookeeper集群&gt; 集群搭建编辑kafka.yml配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374version: '2'services: kafka1: image: wurstmeister/kafka container_name: kafka1 restart: always ports: - "9092" expose: - "9092" environment: KAFKA_BROKER_ID: 1001 KAFKA_BROKER_ID_GENERATION_ENABLE: "false" KAFKA_DELETE_TOPIC_ENABLE: "true" KAFAK_HOST_NAME: kafka1 KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" KAFKA_ADVERTISED_HOST_NAME: kafka1 KAFKA_ADVERTISED_PORT: 9092 JMX_PORT: 9999 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - "./storge/kafka1:/opt/kafka/data:rw" kafka2: image: wurstmeister/kafka container_name: kafka2 restart: always ports: - "9092" expose: - "9092" environment: KAFKA_BROKER_ID: 1002 KAFKA_BROKER_ID_GENERATION_ENABLE: "false" KAFKA_DELETE_TOPIC_ENABLE: "true" KAFAK_HOST_NAME: kafka2 KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" KAFKA_ADVERTISED_HOST_NAME: kafka2 KAFKA_ADVERTISED_PORT: 9092 JMX_PORT: 9999 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - "./storge/kafka2:/opt/kafka/data:rw" kafka3: image: wurstmeister/kafka container_name: kafka3 restart: always ports: - "9092" expose: - "9092" environment: KAFKA_BROKER_ID: 1003 KAFKA_BROKER_ID_GENERATION_ENABLE: "false" KAFKA_DELETE_TOPIC_ENABLE: "true" KAFAK_HOST_NAME: kafka3 KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" KAFKA_ADVERTISED_HOST_NAME: kafka3 KAFKA_ADVERTISED_PORT: 9092 JMX_PORT: 9999 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - "./storge/kafka3:/opt/kafka/data:rw" kafka-manager: image: sheepkiller/kafka-manager restart: always container_name: kafka-manager ports: - "9000:9000" expose: - "9000" environment: HOSTNAME: kafka-manager APPLICATION_SECRET: letmein ZK_HOSTS: zoo1:2181,zoo2:2181,zoo3:2181 运行服务123456789101112➜ docker docker-compose -f kafka.yml up -dkafka-manager is up-to-datekafka1 is up-to-datekafka2 is up-to-datekafka3 is up-to-date➜ docker docker-compose -f kafka.yml ps Name Command State Ports--------------------------------------------------------------------------kafka-manager ./start-kafka-manager.sh Up 0.0.0.0:9000-&gt;9000/tcpkafka1 start-kafka.sh Up 0.0.0.0:32813-&gt;9092/tcpkafka2 start-kafka.sh Up 0.0.0.0:32814-&gt;9092/tcpkafka3 start-kafka.sh Up 0.0.0.0:32815-&gt;9092/tcp 使用方式kafka-manger web界面 地址：http://127.0.0.1:9000 说明 集群包含三个kafka节点:kafka1,kafka2,kafka3,分别监听docker端口为2181 kafka集群依赖zookeeper集群(zoo1,zoo2,zoo3) zookeeper kafka-manager监听在本地9000端口,通过http://127.0.0.1:9000访问 几个关键参数说明 KAFKA_BROKER_ID: 1001 BROKERID作为KAFA节点的唯一标示,在docker环境下我们显式指定,避免在容器重建的时候BROKERID变化 KAFKA_BROKER_ID_GENERATION_ENABLE: &quot;false&quot; _默认为true,我们需要显式指定BROKERID,设置为false KAFKA_DELETE_TOPIC_ENABLE: &quot;false&quot; 开启删除topic功能,方便在zk-manager中进行删除topic操作 KAFAK_HOST_NAME: kafka 此处使用docker主机名 KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;false&quot; 默认为true,我们使用zk-manager进行kafka的topic管理,不允许自动创建 KAFKA_ADVERTISED_HOST_NAME: kafka broker监听域名,这个非常重要,此地址会被注册到zookeeper,请确保docker容器内可以访问 KAFKA_ADVERTISED_PORT: 9092 broker监听端口,重要性同上 JMX_PORT: 9999 开启jmx以便z-manager监控broker状态 ZK_HOSTS: zoo1:2181,zoo2:2181,zoo3:2181 zookeeper地址,这个配置不用多说了 连接到kafka创建测试用topic首先,我们先访问kafka-manager(http:127.0.0.1:9000) 创建一个cluster和一个topictest : 1 Replication : 1 Partitions 安装kafkacat我们使用kafkacat命令行工具作为Produce，Consume演示工具,我们需要访问docker环境内的kafka集群,所以我们用一个带有ENTRYPOINT功能的镜像1docker pull ryane/kafkacat 在使用kafkacat ENTRYPOINT时候需要加入--net=docker_default 准备测试数据现在我们准备一些数据,kafka消息数据至少需要一个key和一个value,我们使用mockjs来做随机数据生成 安装mockjs1npm install nockjs -g 创建测试数据目录1mkdir data 执行生成数据命令进入data目录1repeat 100 echo "`random color`,`random guid`" &gt;&gt; test.csv 此名称生成一个随机的颜色数值和一个随机的guid并且用”,”连接,重复100遍,100遍啊,100遍ヽ(*´∀｀)ノ 查看结果1234567891011tail -f test.csv#9bf279,Ba50fEc5-f7CB-f56b-1FA5-0Ee6c79FA51C#ebf279,cf9FB33f-ca2F-B7f9-0D79-EC0bFF5d5c79#f28479,e24f4823-ccd9-Ad46-2AC0-303FB7cbbC6B#f279f2,6bBDC57b-DEA2-cdfF-b78a-35a6189F9Ffd#7994f2,7aBdfDfF-583A-9CBF-EF62-CEFBddee3982#79cef2,FF9dbEDF-9BC2-Fe9f-A08C-40eFdF578e7D#7987f2,8dAbbaBC-F5f6-cc67-1Fb3-536D21ec5E74#e279f2,37c71A49-ec38-BC0c-7cFF-B4EE8d5ceCFC#f2c679,aD23b7FA-003a-B5E4-78C4-FAC2bb1Ecb2E#8b79f2,4Ff1dAF2-9b7c-FCe7-d66c-CC49972C0cd0 测试数据生成是另一个独立的问题,相关的工具和方法有时间再分享 Produce实例运行命令1docker run --net=docker_default -v="$PWD/data/test.csv:/test.csv" --rm -it ryane/kafkacat -P -b kafka1:9092,kafka2:9092,kafka3:9092 -t test -l /test.csv -K , 此命令会开启一个kafkacat ENTRYPOINT镜像,发送test.csv数据到test topic,一行一条消息,keyvalue以”,”分割。 Consumer实例运行命令123456789101112docker run --net=docker_default --rm -it ryane/kafkacat -C -b kafka1:9092,kafka2:9092,kafka3:9092 -t test -o -10 -f 'Topic:%t partion:%p offset:%o key:%k value:%s\n'Topic:test partion:0 offset:91 key:#9bf279 value:Ba50fEc5-f7CB-f56b-1FA5-0Ee6c79FA51CTopic:test partion:0 offset:92 key:#ebf279 value:cf9FB33f-ca2F-B7f9-0D79-EC0bFF5d5c79Topic:test partion:0 offset:93 key:#f28479 value:e24f4823-ccd9-Ad46-2AC0-303FB7cbbC6BTopic:test partion:0 offset:94 key:#f279f2 value:6bBDC57b-DEA2-cdfF-b78a-35a6189F9FfdTopic:test partion:0 offset:95 key:#7994f2 value:7aBdfDfF-583A-9CBF-EF62-CEFBddee3982Topic:test partion:0 offset:96 key:#79cef2 value:FF9dbEDF-9BC2-Fe9f-A08C-40eFdF578e7DTopic:test partion:0 offset:97 key:#7987f2 value:8dAbbaBC-F5f6-cc67-1Fb3-536D21ec5E74Topic:test partion:0 offset:98 key:#e279f2 value:37c71A49-ec38-BC0c-7cFF-B4EE8d5ceCFCTopic:test partion:0 offset:99 key:#f2c679 value:aD23b7FA-003a-B5E4-78C4-FAC2bb1Ecb2ETopic:test partion:0 offset:100 key:#8b79f2 value:4Ff1dAF2-9b7c-FCe7-d66c-CC49972C0cd0% Reached end of topic test [0] at offset 101 从test topic中获取最后10个消息并且打印信息 开启新窗口多次执行producer命令,consumer会持续的打印输出。 结语至此我们已经使用搭建了一个基本的kafka集群系统,以后会增加一下集群运维和应用场景的分享。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker-compose</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用docker创建zookeeper集群]]></title>
    <url>%2F2017%2F03%2F01%2Fdocker-zookeeper-cluster%2F</url>
    <content type="text"><![CDATA[使用docker创建一个包含三个节点的zookeeper集群,并提供web管理界面。 过去几年接触过的分布式系统大多使用zookeeper作为分布式程序协调服务。去年做solrCloud就有用到,当时是使用单机模式与伪分布式模式,。最近研究kafka也要用到zookeerper,于是在本地使用docker与docker-compose搭建了一个分布式zookeerper集群,附带一个node-zk-browser管理器,以后本地测试与验证终于可以在集群环境下进行了。 原料 Docker version 1.13.0 docker-compose version 1.10.0 fify/node-zk-browser:latest 镜像 zookeeper:latest 镜像 步骤编辑zookeeper.yml配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172version: '2'networks: zookeeper_network: driver: bridgeservices: zoo1: image: zookeeper:latest container_name: zoo1 ports: - "21811:2181" expose: - "2888" - "3888" networks: - "zookeeper_network" volumes: - "./storge/zoo1/data:/data:rw" - "./storge/zoo1/datalog:/datalog:rw" - "./storge/zoo1/conf:/conf:rw" environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo2: image: zookeeper:latest container_name: zoo2 ports: - "21812:2181" networks: - "zookeeper_network" expose: - "2888" - "3888" volumes: - "./storge/zoo2/data:/data:rw" - "./storge/zoo2/datalog:/datalog:rw" - "./storge/zoo2/conf:/conf:rw" environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo3: image: zookeeper:latest container_name: zoo3 ports: - "21813:2181" networks: - "zookeeper_network" expose: - "2888" - "3888" volumes: - "./storge/zoo3/data:/data:rw" - "./storge/zoo3/datalog:/datalog:rw" - "./storge/zoo3/conf:/conf:rw" environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zk-manager: image: fify/node-zk-browser:latest container_name: zk-manager restart: always depends_on: - zoo1 - zoo2 - zoo3 ports: - "3000:3000" networks: - "zookeeper_network" expose: - "3000" environment: ZK_HOST: zoo1:2181,zoo2:2181,zoo3:2181 运行服务123456789101112➜ docker docker-compose -f zookeeper.yml up --remove -dStarting zoo3Starting zoo2Starting zoo1Starting zk-manager➜ docker docker-compose -f zookeeper.yml ps Name Command State Ports-------------------------------------------------------------------------------------------------zk-manager /opt/node-zk-browser/start.sh Up 0.0.0.0:3000-&gt;3000/tcpzoo1 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:21811-&gt;2181/tcp, 2888/tcp, 3888/tcpzoo2 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:21812-&gt;2181/tcp, 2888/tcp, 3888/tcpzoo3 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:21813-&gt;2181/tcp, 2888/tcp, 3888/tcp 使用方式通过zkCli访问: 123456789➜ docker zkCli -server 127.0.0.1:21811,127.0.0.1:21812,127.0.0.1:21813Connecting to 127.0.0.1:21811,127.0.0.1:21812,127.0.0.1:21813Welcome to ZooKeeper!JLine support is enabledWATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 127.0.0.1:21811,127.0.0.1:21812,127.0.0.1:21813(CONNECTED) 0] node-zk-browser web界面 地址：http://127.0.0.1:3000 说明 集群包含三个zookeeper节点:zoo1,zoo2,zoo3,分别监听本地21811,21812,21813三个端口,docker端口为2181 各个zookeeper节点数据存储在本地./storge/zoo1,zoo2,zoo3。 zookeeper节点通过docker端口2888,3888进行通讯,2888为选举端口,3888为备选端口 node-zk-browser通过docker端口2181访问各个zookeeper节点,本地监听3000端口 遗留问题 node-zk-browser界面丑出天际有没有(ノ ﾟДﾟ)ノ ＝＝＝＝ ┻━━┻?谁有更好的选择推荐一下。 目前无法使用docker-compose scale动态扩展zookeeper节点,因为ZOO_MY_ID和ZOO_SERVERS选项需要在配置文件中制定,但这一方面并不是很重要,一般情况下并不需要扩展节点。]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker-compose</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
</search>
