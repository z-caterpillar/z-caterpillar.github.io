<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[kafka java客户端实例]]></title>
      <url>%2F2017%2F03%2F10%2Fkafka-java-client%2F</url>
      <content type="text"><![CDATA[kafa java 客户端实例。实例中包含producer,consumer的几种场景。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[spring-boot微框架创建java docker应用]]></title>
      <url>%2F2017%2F03%2F10%2Fdocker-spring-boot%2F</url>
      <content type="text"><![CDATA[使用sping-boot用最简单的方式发布一个包含http接口的docker服务。 目前项目上本地和测试环境中全部的中间件与server都是部署在docker环境,调试过程中需要将client端代码发布到docker container中运行,spring依赖注入的特性可以简化开发,但是xml配置非常繁琐,spring boot很好的解决了这个问题,下面我们用spring boot做一个简单的docker应用。 原料 Docker version 1.13.0 bankmonitor/spring-boot 镜像 spring boot v1.5.2.RELEASE使用spring boot微框架初始化项目初始化项目:123spring init http-demoUsing service at https://start.spring.ioProject extracted to '/Users/zhaoliang/project/http-demo' 该命令生成了一个包含spring基本组件的项目,目录结构不写了,标准的java项目,只说一下application.properties. 这个文件是spring boot的应用配置文件(说明见spring-boot-doc),我们添加一个server.port=9527,这个配置会修改嵌入tomcat的监听端口,application.properties中的配置内容可以通过@Value(&quot;{key}&quot;)在spring boot中引用,下面的例子会讲。 coding进入项目目录添加新类DemoApplication.java内容如下：12345678910111213141516171819202122232425262728package com.example;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@SpringBootApplication@EnableAutoConfiguration@RestControllerpublic class DemoApplication implements CommandLineRunner &#123; @Value("$&#123;server.port&#125;") public Integer port; @RequestMapping(value = "/hi") public String hi() &#123; return "helloWorld:" + this.port; &#125; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125; @Override public void run(String... args) throws Exception &#123; System.out.println("服务启动"); &#125;&#125; 说明:访问/hi服务端会返回”helloworld” 和我们在application.properties配置的端口信息.实现CommandLineRunner 在服务启动时候执行加载一些数据。 package &amp;&amp; run运行命令:12345678910111213mvn clean package[INFO][INFO] --- maven-jar-plugin:2.6:jar (default-jar) @ http-demo ---[INFO] Building jar: /Users/zhaoliang/project/http-demo/target/http-demo-0.0.1-SNAPSHOT.jar[INFO][INFO] --- spring-boot-maven-plugin:1.5.2.RELEASE:repackage (default) @ http-demo ---[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 4.690 s[INFO] Finished at: 2017-03-10T14:26:57+08:00[INFO] Final Memory: 28M/312M[INFO] ------------------------------------------------------------------------ ./target/http-demo-0.0.1-SNAPSHOT.jar为打包结果。 本地运行:1java -jar target/http-demo-0.0.1-SNAPSHOT.jar 查看结果:12curl http://127.0.0.1:9527/hihelloWorld:9527% 使用容器运行我们使用bankmonitor/spring-boot镜像来运行spring boot程序,镜像启动时候会执行如下cmd:1CMD [\"/bin/sh\" \"-c\" \"java $JAVA_OPTS -jar /app/app.jar --spring.profiles.active=$SPRING_PROFILES_ACTIVE\"] 这就简单了,我们只需要在容器启动时将./target/http-demo-0.0.1-SNAPSHOT.jar挂载到容器文件/app/app.jar即可运行命令:1docker run --rm -v "$PWD/target/http-demo-0.0.1-SNAPSHOT.jar:/app/app.jar" -p "10086:9527" bankmonitor/spring-boot 我们把容器9527端口映射到本地10086端口（´➰｀) 查看结果:12curl http://127.0.0.1:10086/hihelloWorld:10086% 结语spring-boot非常适合作为微框架,并且非常简单,简化了大分布的配置操作,再今后如有使用到新的特性与依赖,会再做分享。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用docker创建kafka集群]]></title>
      <url>%2F2017%2F03%2F02%2Fdocker-kafka-cluster%2F</url>
      <content type="text"><![CDATA[如何使用docker创建一个包含三个节点的kafka集群(非伪分布式),并提供web管理界面。 近期在项目上使用阿里datahub做存储用户行为并且做流式计算,kafka也是大数据解决方案中的一部分,接下来准备从kafka的环境搭建开始,分享一些相关内容。 原料 Docker version 1.13.0 docker-compose version 1.10.0 wurstmeister/kafka 镜像 服务端 ryane/kafkacat 镜像 客户端 sheepkiller/kafka-manager 镜像 管理工具kafka-manager是雅虎推出的kafka管理器偏重于集群与topic管理 zookeeper 集群 搭建方式见&lt;使用docker创建zookeeper集群&gt; 集群搭建编辑kafka.yml配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374version: '2'services: kafka1: image: wurstmeister/kafka container_name: kafka1 restart: always ports: - "9092" expose: - "9092" environment: KAFKA_BROKER_ID: 1001 KAFKA_BROKER_ID_GENERATION_ENABLE: "false" KAFKA_DELETE_TOPIC_ENABLE: "true" KAFAK_HOST_NAME: kafka1 KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" KAFKA_ADVERTISED_HOST_NAME: kafka1 KAFKA_ADVERTISED_PORT: 9092 JMX_PORT: 9999 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - "./storge/kafka1:/opt/kafka/data:rw" kafka2: image: wurstmeister/kafka container_name: kafka2 restart: always ports: - "9092" expose: - "9092" environment: KAFKA_BROKER_ID: 1002 KAFKA_BROKER_ID_GENERATION_ENABLE: "false" KAFKA_DELETE_TOPIC_ENABLE: "true" KAFAK_HOST_NAME: kafka2 KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" KAFKA_ADVERTISED_HOST_NAME: kafka2 KAFKA_ADVERTISED_PORT: 9092 JMX_PORT: 9999 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - "./storge/kafka2:/opt/kafka/data:rw" kafka3: image: wurstmeister/kafka container_name: kafka3 restart: always ports: - "9092" expose: - "9092" environment: KAFKA_BROKER_ID: 1003 KAFKA_BROKER_ID_GENERATION_ENABLE: "false" KAFKA_DELETE_TOPIC_ENABLE: "true" KAFAK_HOST_NAME: kafka3 KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false" KAFKA_ADVERTISED_HOST_NAME: kafka3 KAFKA_ADVERTISED_PORT: 9092 JMX_PORT: 9999 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 volumes: - "./storge/kafka3:/opt/kafka/data:rw" kafka-manager: image: sheepkiller/kafka-manager restart: always container_name: kafka-manager ports: - "9000:9000" expose: - "9000" environment: HOSTNAME: kafka-manager APPLICATION_SECRET: letmein ZK_HOSTS: zoo1:2181,zoo2:2181,zoo3:2181 运行服务123456789101112➜ docker docker-compose -f kafka.yml up -dkafka-manager is up-to-datekafka1 is up-to-datekafka2 is up-to-datekafka3 is up-to-date➜ docker docker-compose -f kafka.yml ps Name Command State Ports--------------------------------------------------------------------------kafka-manager ./start-kafka-manager.sh Up 0.0.0.0:9000-&gt;9000/tcpkafka1 start-kafka.sh Up 0.0.0.0:32813-&gt;9092/tcpkafka2 start-kafka.sh Up 0.0.0.0:32814-&gt;9092/tcpkafka3 start-kafka.sh Up 0.0.0.0:32815-&gt;9092/tcp 使用方式kafka-manger web界面 地址：http://127.0.0.1:9000 说明 集群包含三个kafka节点:kafka1,kafka2,kafka3,分别监听docker端口为2181 kafka集群依赖zookeeper集群(zoo1,zoo2,zoo3) zookeeper kafka-manager监听在本地9000端口,通过http://127.0.0.1:9000访问 几个关键参数说明 KAFKA_BROKER_ID: 1001 BROKERID作为KAFA节点的唯一标示,在docker环境下我们显式指定,避免在容器重建的时候BROKERID变化 KAFKA_BROKER_ID_GENERATION_ENABLE: &quot;false&quot; _默认为true,我们需要显式指定BROKERID,设置为false KAFKA_DELETE_TOPIC_ENABLE: &quot;false&quot; 开启删除topic功能,方便在zk-manager中进行删除topic操作 KAFAK_HOST_NAME: kafka 此处使用docker主机名 KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;false&quot; 默认为true,我们使用zk-manager进行kafka的topic管理,不允许自动创建 KAFKA_ADVERTISED_HOST_NAME: kafka broker监听域名,这个非常重要,此地址会被注册到zookeeper,请确保docker容器内可以访问 KAFKA_ADVERTISED_PORT: 9092 broker监听端口,重要性同上 JMX_PORT: 9999 开启jmx以便z-manager监控broker状态 ZK_HOSTS: zoo1:2181,zoo2:2181,zoo3:2181 zookeeper地址,这个配置不用多说了 连接到kafka创建测试用topic首先,我们先访问kafka-manager(http:127.0.0.1:9000) 创建一个cluster和一个topictest : 1 Replication : 1 Partitions 安装kafkacat我们使用kafkacat命令行工具作为Produce，Consume演示工具,我们需要访问docker环境内的kafka集群,所以我们用一个带有ENTRYPOINT功能的镜像1docker pull ryane/kafkacat 在使用kafkacat ENTRYPOINT时候需要加入--net=docker_default 准备测试数据现在我们准备一些数据,kafka消息数据至少需要一个key和一个value,我们使用mockjs来做随机数据生成 安装mockjs1npm install nockjs -g 创建测试数据目录1mkdir data 执行生成数据命令进入data目录1repeat 100 echo "`random color`,`random guid`" &gt;&gt; test.csv 此名称生成一个随机的颜色数值和一个随机的guid并且用”,”连接,重复100遍,100遍啊,100遍ヽ(*´∀｀)ノ 查看结果1234567891011tail -f test.csv#9bf279,Ba50fEc5-f7CB-f56b-1FA5-0Ee6c79FA51C#ebf279,cf9FB33f-ca2F-B7f9-0D79-EC0bFF5d5c79#f28479,e24f4823-ccd9-Ad46-2AC0-303FB7cbbC6B#f279f2,6bBDC57b-DEA2-cdfF-b78a-35a6189F9Ffd#7994f2,7aBdfDfF-583A-9CBF-EF62-CEFBddee3982#79cef2,FF9dbEDF-9BC2-Fe9f-A08C-40eFdF578e7D#7987f2,8dAbbaBC-F5f6-cc67-1Fb3-536D21ec5E74#e279f2,37c71A49-ec38-BC0c-7cFF-B4EE8d5ceCFC#f2c679,aD23b7FA-003a-B5E4-78C4-FAC2bb1Ecb2E#8b79f2,4Ff1dAF2-9b7c-FCe7-d66c-CC49972C0cd0 测试数据生成是另一个独立的问题,相关的工具和方法有时间再分享 Produce实例运行命令1docker run --net=docker_default -v="$PWD/data/test.csv:/test.csv" --rm -it ryane/kafkacat -P -b kafka1:9092,kafka2:9092,kafka3:9092 -t test -l /test.csv -K , 此命令会开启一个kafkacat ENTRYPOINT镜像,发送test.csv数据到test topic,一行一条消息,keyvalue以”,”分割。 Consumer实例运行命令123456789101112docker run --net=docker_default --rm -it ryane/kafkacat -C -b kafka1:9092,kafka2:9092,kafka3:9092 -t test -o -10 -f 'Topic:%t partion:%p offset:%o key:%k value:%s\n'Topic:test partion:0 offset:91 key:#9bf279 value:Ba50fEc5-f7CB-f56b-1FA5-0Ee6c79FA51CTopic:test partion:0 offset:92 key:#ebf279 value:cf9FB33f-ca2F-B7f9-0D79-EC0bFF5d5c79Topic:test partion:0 offset:93 key:#f28479 value:e24f4823-ccd9-Ad46-2AC0-303FB7cbbC6BTopic:test partion:0 offset:94 key:#f279f2 value:6bBDC57b-DEA2-cdfF-b78a-35a6189F9FfdTopic:test partion:0 offset:95 key:#7994f2 value:7aBdfDfF-583A-9CBF-EF62-CEFBddee3982Topic:test partion:0 offset:96 key:#79cef2 value:FF9dbEDF-9BC2-Fe9f-A08C-40eFdF578e7DTopic:test partion:0 offset:97 key:#7987f2 value:8dAbbaBC-F5f6-cc67-1Fb3-536D21ec5E74Topic:test partion:0 offset:98 key:#e279f2 value:37c71A49-ec38-BC0c-7cFF-B4EE8d5ceCFCTopic:test partion:0 offset:99 key:#f2c679 value:aD23b7FA-003a-B5E4-78C4-FAC2bb1Ecb2ETopic:test partion:0 offset:100 key:#8b79f2 value:4Ff1dAF2-9b7c-FCe7-d66c-CC49972C0cd0% Reached end of topic test [0] at offset 101 从test topic中获取最后10个消息并且打印信息 开启新窗口多次执行producer命令,consumer会持续的打印输出。 结语至此我们已经使用搭建了一个基本的kafka集群系统,以后会增加一下集群运维和应用场景的分享。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用docker创建zookeeper集群]]></title>
      <url>%2F2017%2F03%2F01%2Fdocker-zookeeper-cluster%2F</url>
      <content type="text"><![CDATA[使用docker创建一个包含三个节点的zookeeper集群,并提供web管理界面。 过去几年接触过的分布式系统大多使用zookeeper作为分布式程序协调服务。去年做solrCloud就有用到,当时是使用单机模式与伪分布式模式,。最近研究kafka也要用到zookeerper,于是在本地使用docker与docker-compose搭建了一个分布式zookeerper集群,附带一个node-zk-browser管理器,以后本地测试与验证终于可以在集群环境下进行了。 原料 Docker version 1.13.0 docker-compose version 1.10.0 fify/node-zk-browser:latest 镜像 zookeeper:latest 镜像 步骤编辑zookeeper.yml配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172version: '2'networks: zookeeper_network: driver: bridgeservices: zoo1: image: zookeeper:latest container_name: zoo1 ports: - "21811:2181" expose: - "2888" - "3888" networks: - "zookeeper_network" volumes: - "./storge/zoo1/data:/data:rw" - "./storge/zoo1/datalog:/datalog:rw" - "./storge/zoo1/conf:/conf:rw" environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo2: image: zookeeper:latest container_name: zoo2 ports: - "21812:2181" networks: - "zookeeper_network" expose: - "2888" - "3888" volumes: - "./storge/zoo2/data:/data:rw" - "./storge/zoo2/datalog:/datalog:rw" - "./storge/zoo2/conf:/conf:rw" environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo3: image: zookeeper:latest container_name: zoo3 ports: - "21813:2181" networks: - "zookeeper_network" expose: - "2888" - "3888" volumes: - "./storge/zoo3/data:/data:rw" - "./storge/zoo3/datalog:/datalog:rw" - "./storge/zoo3/conf:/conf:rw" environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zk-manager: image: fify/node-zk-browser:latest container_name: zk-manager restart: always depends_on: - zoo1 - zoo2 - zoo3 ports: - "3000:3000" networks: - "zookeeper_network" expose: - "3000" environment: ZK_HOST: zoo1:2181,zoo2:2181,zoo3:2181 运行服务123456789101112➜ docker docker-compose -f zookeeper.yml up --remove -dStarting zoo3Starting zoo2Starting zoo1Starting zk-manager➜ docker docker-compose -f zookeeper.yml ps Name Command State Ports-------------------------------------------------------------------------------------------------zk-manager /opt/node-zk-browser/start.sh Up 0.0.0.0:3000-&gt;3000/tcpzoo1 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:21811-&gt;2181/tcp, 2888/tcp, 3888/tcpzoo2 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:21812-&gt;2181/tcp, 2888/tcp, 3888/tcpzoo3 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:21813-&gt;2181/tcp, 2888/tcp, 3888/tcp 使用方式通过zkCli访问: 123456789➜ docker zkCli -server 127.0.0.1:21811,127.0.0.1:21812,127.0.0.1:21813Connecting to 127.0.0.1:21811,127.0.0.1:21812,127.0.0.1:21813Welcome to ZooKeeper!JLine support is enabledWATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 127.0.0.1:21811,127.0.0.1:21812,127.0.0.1:21813(CONNECTED) 0] node-zk-browser web界面 地址：http://127.0.0.1:3000 说明 集群包含三个zookeeper节点:zoo1,zoo2,zoo3,分别监听本地21811,21812,21813三个端口,docker端口为2181 各个zookeeper节点数据存储在本地./storge/zoo1,zoo2,zoo3。 zookeeper节点通过docker端口2888,3888进行通讯,2888为选举端口,3888为备选端口 node-zk-browser通过docker端口2181访问各个zookeeper节点,本地监听3000端口 遗留问题 node-zk-browser界面丑出天际有没有(ノ ﾟДﾟ)ノ ＝＝＝＝ ┻━━┻?谁有更好的选择推荐一下。 目前无法使用docker-compose scale动态扩展zookeeper节点,因为ZOO_MY_ID和ZOO_SERVERS选项需要在配置文件中制定,但这一方面并不是很重要,一般情况下并不需要扩展节点。]]></content>
    </entry>

    
  
  
</search>
